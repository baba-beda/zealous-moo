- split training data into two parts: train (70%) and test (30%), it's better to split randomly. this way you can see if you hypotesis function is overfitting the data
  1. get theta from the first part
  2. compute test error on the second part of theta / get misclassification error
- model selection
  * what degree of polinomial?
    1. for each variant of model get theta
    2. look at test error within each mode
  * how well does the model generalize to new examples?
    1. split data into three pieces: training set (60%), cross validation set (20%), test set (20%)
    2. train cost function using training set for each model
    3. test this theta on cross-validation set
    4. use model, that gave best results on cross-valudation and look how it's generalizes on test set
  * plot error = function(degree of polynomial) for training error and cross-validation error
    1. on the plot both errors will be high in case of underfitting
    2. cross-validation will be high and training error will be low in case of overfitting
  * regularization
    1. if lambda is high => underfit, if small => overfit
    2. try several values of lambda, get different variants of theta from training set
    3. test thetas on cross-validation set
    4. use general cost function with regulariation component, and train and cv cost functions withoестьut it, plot errors of latters as function of lambda.
  * size of training set
    1. construct several models, using chunks of data of different sizes
    2. plot training and cv cost functions as function of set size
- options:
  1. get more training examples => fix high variance
  2. try smaller sets of features => fix high variance
  3. try getting additional features => fix high bias
  4. try adding polynomial features => fix high bias
  5. try decreasing lambda => fix high bias
  6. try increasing lambda => fix high variance
- neural networks 
  * small networks => possible underfit
  * large networks => overfit

